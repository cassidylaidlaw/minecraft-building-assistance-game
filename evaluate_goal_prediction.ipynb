{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File utils\n",
    "\n",
    "\n",
    "def get_max_run_dir_with_checkpoint(path: Path):\n",
    "    \"\"\"Return the directory with the highest run ID that contains a checkpoint.\n",
    "\n",
    "    If there are no directories with a checkpoint, return None.\n",
    "    \"\"\"\n",
    "    run_dirs = [\n",
    "        d for d in path.iterdir() if d.name.isdigit() and any(d.glob(\"checkpoint_*\"))\n",
    "    ]\n",
    "    if not run_dirs:\n",
    "        return None\n",
    "\n",
    "    return max(\n",
    "        run_dirs,\n",
    "        key=lambda d: int(d.name),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_checkpoint_paths(run_dir, checkpoints_to_eval, no_eval_results=False):\n",
    "    \"\"\"Return checkpoint paths to evaluate.\n",
    "\n",
    "    Args:\n",
    "        run_dir: Path to the run directory in which checkpoints are stored.\n",
    "        checkpoints_to_eval: List of checkpoint numbers to evaluate.\n",
    "        no_eval_results: If True, only return checkpoints that do not have\n",
    "            evaluation results. Otherwise, return all checkpoints.\n",
    "    \"\"\"\n",
    "    checkpoint_paths = []\n",
    "    for checkpoint in checkpoints_to_eval:\n",
    "        checkpoint_path = run_dir / f\"checkpoint_{str(checkpoint).zfill(6)}\"\n",
    "        if checkpoint_path.exists():\n",
    "            if not no_eval_results or not any(\n",
    "                checkpoint_path.glob(\"**/episode_results.pickle\")\n",
    "            ):\n",
    "                checkpoint_paths.append(checkpoint_path)\n",
    "    return checkpoint_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start eval runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_filter = \"**/2024-09-*\"\n",
    "base_dirs = [\n",
    "    \"/nas/ucb/ebronstein/minecraft-building-assistance-game/data/logs/MbagAlphaZero/2_players/11x10x10/craftassist/teleportation_False/inf_blocks_True/human_bc_combined_20240903_20\",\n",
    "    \"/nas/ucb/ebronstein/minecraft-building-assistance-game/data/logs/MbagAlphaZero/2_players/11x10x10/craftassist/teleportation_False/inf_blocks_True/human_bc_6x64_lr_0.001_20240703_20\",\n",
    "]\n",
    "\n",
    "exp_dirs = []\n",
    "for base_dir in base_dirs:\n",
    "    exp_dirs.extend(list(Path(base_dir).glob(base_dir_filter)))\n",
    "\n",
    "run_dirs = [get_max_run_dir_with_checkpoint(exp_dir) for exp_dir in exp_dirs]\n",
    "run_dirs = [d for d in run_dirs if d is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate every 100 checkpoints.\n",
    "checkpoints_to_eval = list(range(100, 2100, 100))\n",
    "no_eval_results = True\n",
    "\n",
    "checkpoint_paths = []\n",
    "for run_dir in run_dirs:\n",
    "    checkpoint_paths += get_checkpoint_paths(\n",
    "        run_dir, checkpoints_to_eval, no_eval_results=no_eval_results\n",
    "    )\n",
    "\n",
    "# Check that no evaluation results exist if no_eval_results is True.\n",
    "if no_eval_results:\n",
    "    for path in checkpoint_paths:\n",
    "        assert not any(path.glob(\"**/episode_results.pickle\"))\n",
    "\n",
    "len(checkpoint_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation commands.\n",
    "\n",
    "qos = \"scavenger\"\n",
    "partition = \"scavenger\" if qos == \"scavenger\" else None\n",
    "\n",
    "for checkpoint_path in checkpoint_paths:\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(checkpoint_path)\n",
    "\n",
    "    partition_str = \"\" if partition is None else f\" --partition={partition}\"\n",
    "    requeue_str = \" --requeue\" if qos == \"scavenger\" else \"\"\n",
    "    print(\n",
    "        f\"CHECKPOINT={checkpoint_path} sbatch --qos={qos}{partition_str}{requeue_str} scripts/slurm_eval_goal_pred.sh\",\n",
    "        end=\"\\n\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "\n",
    "def get_params_from_checkpoint_path(checkpoint_path):\n",
    "    param_prefixes = [\n",
    "        \"human\",  # Human model\n",
    "        \"batch\",\n",
    "        \"horizon\",\n",
    "        \"rollout\",\n",
    "        \"sgd_minibatch\",\n",
    "        \"replay\",\n",
    "        \"model_replay\",\n",
    "        \"train\",\n",
    "        \"max_seq_len\",\n",
    "        \"gamma\",\n",
    "        \"lr\",\n",
    "        \"weight_decay\",\n",
    "        \"model\",\n",
    "        \"dim_feedforward\",\n",
    "        \"num_heads\",\n",
    "        \"norm_first\",\n",
    "        \"embedding_size\",\n",
    "        \"position_embedding_size\",\n",
    "        \"position_embedding_angle\",\n",
    "        \"grad_clip\",\n",
    "        \"prev_goal_kl\",\n",
    "        \"goal_loss_coeff\",\n",
    "        \"vf_loss_coeff\",\n",
    "        \"other_agent_action_predictor_loss_coeff\",\n",
    "    ]\n",
    "    params = {}\n",
    "\n",
    "    for part in checkpoint_path.parts:\n",
    "        for prefix in param_prefixes:\n",
    "            if part.startswith(prefix):\n",
    "                params[prefix] = part.split(prefix + \"_\")[1]\n",
    "                break\n",
    "\n",
    "        if part == \"interleave_lstm\":\n",
    "            params[\"interleave_lstm\"] = True\n",
    "        elif \"sep_transformer\" in part:\n",
    "            # True for \"sep_transformer\" and False for \"no_sep_transformer\"\n",
    "            params[\"sep_transformer\"] = part.startswith(\"sep_transformer\")\n",
    "\n",
    "    if \"interleave_lstm\" not in params:\n",
    "        params[\"interleave_lstm\"] = False\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def episode_results_to_df(episode_results):\n",
    "    common_keys = (\"cross_entropy\", \"cross_entropy_different\")\n",
    "    per_agent_keys = (\n",
    "        \"cross_entropy_by_last_interacted\",\n",
    "        \"cross_entropy_different_by_last_interacted\",\n",
    "    )\n",
    "    rows = []\n",
    "    for i, episode in enumerate(episode_results):\n",
    "        common_row_dict = {key: episode[key] for key in common_keys}\n",
    "        common_row_dict[\"episode\"] = i\n",
    "        agent_ids = []\n",
    "        for key in per_agent_keys:\n",
    "            agent_ids.extend(list(episode[key].keys()))\n",
    "        agent_ids = set(agent_ids)\n",
    "        for agent_id in agent_ids:\n",
    "            row_dict = common_row_dict.copy()\n",
    "            row_dict[\"agent_id\"] = agent_id\n",
    "            for key in per_agent_keys:\n",
    "                row_dict[key] = episode[key][agent_id]\n",
    "            rows.append(row_dict)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def get_date_from_eval_path(eval_path):\n",
    "    date_str = eval_path.parts[-3][len(\"evaluate_on_\") :].split(\"_\")[-1]\n",
    "    return datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "\n",
    "def load_goal_pred_dfs_for_checkpoint(checkpoint_path):\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    episode_results_paths = list(\n",
    "        checkpoint_path.glob(\"evaluate_on_*/*/episode_results.pickle\")\n",
    "    )\n",
    "\n",
    "    if not episode_results_paths:\n",
    "        return []\n",
    "\n",
    "    paths_and_dates = [\n",
    "        (path, get_date_from_eval_path(path)) for path in episode_results_paths\n",
    "    ]\n",
    "    max_date = max([date for _, date in paths_and_dates])\n",
    "    episode_results_paths = [path for path, date in paths_and_dates if date == max_date]\n",
    "\n",
    "    max_run_id_per_eval = {}\n",
    "    for path in episode_results_paths:\n",
    "        parts = path.parts\n",
    "        eval_id = parts[-3]\n",
    "        run_id = int(parts[-2])\n",
    "        if eval_id not in max_run_id_per_eval:\n",
    "            max_run_id_per_eval[eval_id] = run_id\n",
    "        else:\n",
    "            max_run_id_per_eval[eval_id] = max(max_run_id_per_eval[eval_id], run_id)\n",
    "\n",
    "    dfs = []\n",
    "    for eval_id, run_id in max_run_id_per_eval.items():\n",
    "        episode_result_path = (\n",
    "            checkpoint_path / eval_id / str(run_id) / \"episode_results.pickle\"\n",
    "        )\n",
    "\n",
    "        with open(episode_result_path, \"rb\") as f:\n",
    "            episode_results = pickle.load(f)\n",
    "        df = episode_results_to_df(episode_results)\n",
    "        df[\"split\"] = episode_result_path.parts[-3][len(\"evaluate_on_\") :].split(\"_\")[0]\n",
    "\n",
    "        df[\"checkpoint\"] = int(checkpoint_path.stem.split(\"_\")[1])\n",
    "        df[\"run_path\"] = checkpoint_path.parent\n",
    "\n",
    "        params = get_params_from_checkpoint_path(checkpoint_path)\n",
    "        for key, value in params.items():\n",
    "            df[key] = value\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def assert_df_is_not_duplicated(df, expected_unique_cols=None):\n",
    "    \"\"\"Check that there are no duplicate rows.\"\"\"\n",
    "    if expected_unique_cols is None:\n",
    "        # By default, don't consider the metric columns.\n",
    "        expected_unique_cols = set(df.columns) - {\n",
    "            \"cross_entropy\",\n",
    "            \"cross_entropy_different\",\n",
    "            \"cross_entropy_by_last_interacted\",\n",
    "            \"cross_entropy_different_by_last_interacted\",\n",
    "        }\n",
    "    assert not df.duplicated(subset=expected_unique_cols).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get checkpoint paths for plotting.\n",
    "\n",
    "# Evaluate every 100 checkpoints.\n",
    "checkpoints_to_eval = list(range(100, 2100, 100))\n",
    "no_eval_results = False\n",
    "\n",
    "checkpoint_paths = []\n",
    "for run_dir in run_dirs:\n",
    "    checkpoint_paths += get_checkpoint_paths(\n",
    "        run_dir, checkpoints_to_eval, no_eval_results=no_eval_results\n",
    "    )\n",
    "\n",
    "# Check that no evaluation results exist if no_eval_results is True.\n",
    "if no_eval_results:\n",
    "    for path in checkpoint_paths:\n",
    "        assert not any(path.glob(\"**/episode_results.pickle\"))\n",
    "\n",
    "len(checkpoint_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load episode results from checkpoints.\n",
    "\n",
    "\n",
    "DEFAULT_COL_VALUES = {\n",
    "    \"model_replay\": \"0\",\n",
    "    \"embedding_size\": \"16\",\n",
    "    \"position_embedding_angle\": \"10000\",\n",
    "    \"interleave_lstm\": False,\n",
    "}\n",
    "\n",
    "all_df = []\n",
    "for checkpoint_path in checkpoint_paths:\n",
    "    for df in all_df:\n",
    "        assert checkpoint_path not in df[\"checkpoint\"].values\n",
    "    dfs = load_goal_pred_dfs_for_checkpoint(checkpoint_path)\n",
    "    all_df.extend(dfs)\n",
    "\n",
    "all_df = pd.concat(all_df, ignore_index=True)\n",
    "all_df.fillna(value=DEFAULT_COL_VALUES, inplace=True)\n",
    "\n",
    "assert_df_is_not_duplicated(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_PARAMS = {\n",
    "    \"batch\": \"4096\",\n",
    "    \"horizon\": \"1500\",\n",
    "    \"rollout\": \"64\",\n",
    "    \"sgd_minibatch\": \"1024\",\n",
    "    \"replay\": \"32768\",\n",
    "    \"model_replay\": \"131072\",\n",
    "    \"train\": \"1x64\",\n",
    "    \"max_seq_len\": \"64\",\n",
    "    \"gamma\": \"0.95\",\n",
    "    \"lr\": \"0.001\",\n",
    "    \"weight_decay\": \"0\",\n",
    "    \"model\": \"8x64\",\n",
    "    \"dim_feedforward\": \"64\",\n",
    "    \"num_heads\": \"4\",\n",
    "    \"norm_first\": \"False\",\n",
    "    \"embedding_size\": \"16\",\n",
    "    \"position_embedding_size\": \"48\",\n",
    "    \"position_embedding_angle\": \"10\",\n",
    "    \"grad_clip\": \"10\",\n",
    "    \"prev_goal_kl\": \"3\",\n",
    "    \"goal_loss_coeff\": \"3\",\n",
    "    \"vf_loss_coeff\": \"0.01\",\n",
    "    \"other_agent_action_predictor_loss_coeff\": \"1.0\",\n",
    "    \"interleave_lstm\": True,\n",
    "    \"sep_transformer\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_df.query(\"split == 'train' and episode == 0 and agent_id == 0 and checkpoint == 100 and human == 'bc_combined_20240903_20' and model == '12x256'\")\n",
    "for col in df.columns:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lr.unique(), df.emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue = \"embedding_size\"\n",
    "# Columns of the facet grid. Can be None, which won't create separate columns.\n",
    "col = \"position_embedding_size\"\n",
    "# Parameters to use directly (if use_baseline_params is False) or to update the\n",
    "# baseline params (if use_baseline_params is True).\n",
    "update_params = {\n",
    "    \"human\": \"bc_combined_20240903_20\",\n",
    "    \"model\": \"12x256\",\n",
    "    \"lr\": \"0.001\",\n",
    "    # \"embedding_size\": \"16\",\n",
    "    # \"position_embedding_size\": \"48\",\n",
    "}  # {\"sep_transformer\": False}\n",
    "use_baseline_params = False\n",
    "\n",
    "if use_baseline_params:\n",
    "    col_to_param_dict = BASELINE_PARAMS.copy()\n",
    "    col_to_param_dict.update(update_params)\n",
    "    # Allow multiple values for hue because we want to plot them.\n",
    "    del col_to_param_dict[hue]\n",
    "else:\n",
    "    col_to_param_dict = update_params.copy()\n",
    "\n",
    "assert hue not in col_to_param_dict\n",
    "\n",
    "data = all_df.copy()\n",
    "for key, value in col_to_param_dict.items():\n",
    "    data = data[data[key] == value]\n",
    "\n",
    "# If there are multiple runs per unique hue value, keep only the most recent run.\n",
    "unique_run_data = []\n",
    "group_cols = [hue] if col is None else [hue, col]\n",
    "for key, group_df in data.groupby(group_cols):\n",
    "    run_paths = group_df.run_path.unique()\n",
    "    if len(run_paths) > 1:\n",
    "        most_recent_run_path = max(\n",
    "            run_paths, key=lambda p: datetime.strptime(p.parts[-2], \"%Y-%m-%d_%H-%M-%S\")\n",
    "        )\n",
    "        group_df = group_df[group_df.run_path == most_recent_run_path]\n",
    "    unique_run_data.append(group_df)\n",
    "\n",
    "unique_run_data = pd.concat(unique_run_data, ignore_index=True)\n",
    "\n",
    "for metric in [\n",
    "    \"cross_entropy_different_by_last_interacted\",\n",
    "    \"cross_entropy_by_last_interacted\",\n",
    "]:\n",
    "    sns.relplot(\n",
    "        data=unique_run_data,\n",
    "        x=\"checkpoint\",\n",
    "        y=metric,\n",
    "        row=\"agent_id\",\n",
    "        col=col,\n",
    "        hue=hue,\n",
    "        style=\"split\",\n",
    "        kind=\"line\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sns.relplot(\n",
    "        data=df,\n",
    "        x=\"checkpoint\",\n",
    "        y=metric,\n",
    "        row=\"agent_id\",\n",
    "        hue=x,\n",
    "        style=\"split\",\n",
    "        kind=\"line\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"max_seq_len\"\n",
    "plot_df = all_df.query(\n",
    "    \"model == '8x64' and sep_transformer\"\n",
    ")\n",
    "\n",
    "for y in [\n",
    "    \"cross_entropy\",\n",
    "    \"cross_entropy_different\",\n",
    "    \"cross_entropy_by_last_interacted\",\n",
    "    \"cross_entropy_different_by_last_interacted\",\n",
    "]:\n",
    "    if \"by_last_interacted\" in y:\n",
    "        data = plot_df\n",
    "    else:\n",
    "        data = plot_df[plot_df[\"agent_id\"] == 0]\n",
    "\n",
    "    # expected_unique_cols = {\"episode\", \"agent_id\", \"split\", x}\n",
    "    # assert_df_is_not_duplicated(data, expected_unique_cols=expected_unique_cols)\n",
    "\n",
    "    sns.catplot(\n",
    "        data=data,\n",
    "        kind=\"bar\",\n",
    "        x=x,\n",
    "        order=[str(i) for i in [8, 16, 32, 64, 128]],\n",
    "        y=y,\n",
    "        hue=\"split\",\n",
    "        col=\"agent_id\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"model\"\n",
    "plot_df = all_df.query(\n",
    "    \"max_seq_len == '128' and sep_transformer and dim_feedforward == '64' and \"\n",
    "    \"position_embedding_size == '18' and position_embedding_angle == '10000'\"\n",
    ")\n",
    "\n",
    "for y in [\n",
    "    \"cross_entropy\",\n",
    "    \"cross_entropy_different\",\n",
    "    \"cross_entropy_by_last_interacted\",\n",
    "    \"cross_entropy_different_by_last_interacted\",\n",
    "]:\n",
    "    if \"by_last_interacted\" in y:\n",
    "        data = plot_df\n",
    "    else:\n",
    "        data = plot_df[plot_df[\"agent_id\"] == 0]\n",
    "\n",
    "    # expected_unique_cols = {\"episode\", \"agent_id\", \"split\", x}\n",
    "    # assert_df_is_not_duplicated(data, expected_unique_cols=expected_unique_cols)\n",
    "\n",
    "    sns.catplot(\n",
    "        data=data,\n",
    "        kind=\"bar\",\n",
    "        x=x,\n",
    "        order=[str(i) for i in [8, 16, 32, 64, 128]],\n",
    "        y=y,\n",
    "        hue=\"split\",\n",
    "        col=\"agent_id\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in plot_df.columns:\n",
    "    nunique = plot_df[col].nunique()\n",
    "    if nunique > 1:\n",
    "        print(col, nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"dim_feedforward\", \"position_embedding_size\", \"position_embedding_angle\"]:\n",
    "    print(plot_df[col].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
